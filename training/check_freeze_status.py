"""
å¿«é€Ÿæ£€æŸ¥ CLIP Adapter æ¨¡å‹çš„å‚æ•°å†»ç»“çŠ¶æ€
"""
import sys
import os
sys.path.append('.')

import yaml
import torch
from transformers import CLIPModel
from training.detectors.clip_adapter_detector import CLIPAdapterDetector

# åŠ è½½é…ç½®
with open('training/config/detector/clip_adapter.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# æ·»åŠ å¿…è¦çš„é…ç½®é¡¹
config['lmdb'] = False
config['rgb_dir'] = './datasets/rgb'
config['lmdb_dir'] = './datasets/lmdb'
config['dataset_json_folder'] = 'preprocessing/dataset_json'
config['label_dict'] = {'FF-real': 0, 'FF-F2F': 1, 'FF-DF': 1, 'FF-FS': 1, 'FF-NT': 1}

print("=" * 80)
print("CLIP Adapter å‚æ•°å†»ç»“çŠ¶æ€æ£€æŸ¥")
print("=" * 80)
print(f"å†»ç»“ç­–ç•¥: {config['backbone_config']['mode']}")
print()

# åˆ›å»ºæ¨¡å‹
model = CLIPAdapterDetector(config)

# ç»Ÿè®¡å‚æ•°
total_params = sum(p.numel() for p in model.backbone.parameters())
trainable_backbone = sum(p.numel() for p in model.backbone.parameters() if p.requires_grad)
frozen_backbone = total_params - trainable_backbone

# æ–°æ¨¡å—å‚æ•°
new_modules_params = 0
if model.freq_adapter:
    new_modules_params += sum(p.numel() for p in model.freq_adapter.parameters())
if model.boundary_mining:
    new_modules_params += sum(p.numel() for p in model.boundary_mining.parameters())
if model.head:
    new_modules_params += sum(p.numel() for p in model.head.parameters())

print("ğŸ“Š å‚æ•°ç»Ÿè®¡:")
print(f"  Backbone æ€»å‚æ•°: {total_params:,}")
print(f"  âœ… Backbone å¯è®­ç»ƒ: {trainable_backbone:,} ({100*trainable_backbone/total_params:.2f}%)")
print(f"  âŒ Backbone å†»ç»“: {frozen_backbone:,} ({100*frozen_backbone/total_params:.2f}%)")
print(f"  âœ… æ–°æ¨¡å—å¯è®­ç»ƒ: {new_modules_params:,} (Freq-Adapter + Boundary + Head)")
print(f"  ğŸ“¦ æ€»å¯è®­ç»ƒå‚æ•°: {trainable_backbone + new_modules_params:,}")
print()

print("ğŸ” è¯¦ç»†å†»ç»“çŠ¶æ€:")
print("-" * 80)

# æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡
module_stats = {}
for name, param in model.backbone.named_parameters():
    # æå–æ¨¡å—åï¼ˆä¾‹å¦‚ï¼šencoder.layers.0.self_attn.q_projï¼‰
    parts = name.split('.')
    if len(parts) >= 2:
        module_key = '.'.join(parts[:2])  # ä¾‹å¦‚ï¼šencoder.layers
    else:
        module_key = parts[0]
    
    if module_key not in module_stats:
        module_stats[module_key] = {'trainable': 0, 'frozen': 0}
    
    if param.requires_grad:
        module_stats[module_key]['trainable'] += param.numel()
    else:
        module_stats[module_key]['frozen'] += param.numel()

print("\nâœ… å¯è®­ç»ƒçš„æ¨¡å—:")
for mod, stats in sorted(module_stats.items()):
    if stats['trainable'] > 0:
        total_mod = stats['trainable'] + stats['frozen']
        print(f"  {mod:40s} {stats['trainable']:>12,} / {total_mod:>12,} ({100*stats['trainable']/total_mod:>5.2f}%)")

print("\nâŒ å®Œå…¨å†»ç»“çš„æ¨¡å—:")
for mod, stats in sorted(module_stats.items()):
    if stats['trainable'] == 0 and stats['frozen'] > 0:
        print(f"  {mod:40s} {stats['frozen']:>12,} params")

print("\n" + "=" * 80)
print("ğŸ’¡ æç¤º:")
print("  - å½“å‰ä½¿ç”¨ 'ln_tuning' æ¨¡å¼ï¼Œä»… LayerNorm å±‚å¯è®­ç»ƒ")
print("  - å¦‚éœ€è§£å†»æ›´å¤šå±‚ï¼Œä¿®æ”¹ config ä¸­çš„ mode å’Œ unfreeze_last_n_layers")
print("  - Freq-Adapterã€Boundary Miningã€Head å§‹ç»ˆå¯è®­ç»ƒ")
print("=" * 80)

